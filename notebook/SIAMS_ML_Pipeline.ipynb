{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75f70c7",
   "metadata": {},
   "source": [
    "# SIAMS — End-to-End ML Pipeline (App-Aligned)\n",
    "\n",
    "**Data source:** Multi-sheet Excel, Google Sheets CSV, local CSV  \n",
    "**Timezone:** `Africa/Lagos` (WAT, UTC+1)  \n",
    "**Models:** Decision Tree, Random Forest, Gradient Boosting, XGBoost, Dryness Classifier, t+1 Forecaster  \n",
    "**Outputs:** `model.joblib`, `expected_features.json`, evaluation metrics, plots, all models\n",
    "\n",
    "> This notebook provides a clean, self-contained pipeline aligned with Streamlit app and Google Sheet setup. It includes data exploration, time-aware splits, fixed site encoding, clipped predictions, dryness classification, and short-term forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3b465",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef692ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config you can edit ---\n",
    "# Data sources (set one, leave others empty)\n",
    "MULTISHEET_XLSX = \"SIAMS DataLog.xlsx\"   # the xlsx file with multiple sheets (each sheet = one site)\n",
    "SHEET_CSV_URL   = \"\"   # Google Sheet CSV export URL\n",
    "LOCAL_CSV       = \"\"   # fallback local CSV\n",
    "\n",
    "# Training settings\n",
    "KNOWN_SITES     = [\"Ikorodu\", \"Ogun\", \"Osun\", \"Unilag\"]  # sites for one-hot encoding\n",
    "TEST_FRACTION   = 0.20  # fraction for test split\n",
    "RANDOM_STATE    = 42   # for reproducibility\n",
    "\n",
    "# Dryness classification threshold\n",
    "DRY_THRESHOLD   = 20.0  # Threshold for dryness classification (moisture %)\n",
    "                        # 20% = early stress detection (recommended for most crops)\n",
    "                        # 15% = severe drought stress\n",
    "                        # 30% = conservative (less sensitive to dry conditions)\n",
    "\n",
    "# Feature engineering settings\n",
    "LAG_WINDOWS     = (1, 2, 3, 6)  # Lag periods for soil moisture (hours/readings)\n",
    "ROLLING_WINDOW  = 6             # Rolling average window size\n",
    "LIGHT_ADC_MAX   = 4095          # Maximum ADC value for light sensor inversion\n",
    "\n",
    "# Model hyperparameters (tune for better performance)\n",
    "RF_N_ESTIMATORS = 300           # Random Forest: number of trees\n",
    "RF_MAX_DEPTH    = 15            # Random Forest: max tree depth (None = unlimited)\n",
    "RF_MIN_SAMPLES  = 3             # Random Forest: min samples per leaf\n",
    "\n",
    "XGB_N_ESTIMATORS = 400          # XGBoost: number of boosting rounds\n",
    "XGB_LEARNING_RATE = 0.08        # XGBoost: learning rate\n",
    "XGB_MAX_DEPTH   = 6             # XGBoost: max tree depth\n",
    "\n",
    "CLF_N_ESTIMATORS = 400          # Classifier: number of trees\n",
    "CLF_MAX_DEPTH   = 12            # Classifier: max tree depth\n",
    "\n",
    "# Plant Health Metrics Configuration\n",
    "VPD_DISEASE_THRESHOLD = 0.6     # VPD threshold for disease risk (kPa)\n",
    "HEAT_STRESS_TEMP = 35           # Temperature threshold for heat stress (°C)\n",
    "FROST_RISK_TEMP = 2             # Temperature threshold for frost risk (°C)\n",
    "WATERLOG_THRESHOLD = 85         # Soil moisture threshold for waterlogging (%)\n",
    "DRYSPELL_THRESHOLD = 25         # Soil moisture threshold for dry spell (%)\n",
    "SENSOR_STABILITY_WINDOW = 12    # Rolling window for sensor health check\n",
    "MOISTURE_PATTERN_WINDOW = 6     # Rolling window for moisture patterns\n",
    "\n",
    "# Single source of truth for all health metrics\n",
    "HEALTH_ALL = [\n",
    "    \"vpd_kpa\", \"dew_point_c\", \"heat_flag\", \"frost_flag\",\n",
    "    \"waterlog_flag\", \"dryspell_flag\", \"disease_risk\", \"sensor_issue_flag\"\n",
    "]\n",
    "\n",
    "# Light column headers (add if your data uses different names)\n",
    "LIGHT_HEADERS_INVERTED = [\n",
    "    \"Relative Light Intensity (ADC Units)\",\n",
    "    \"Light Inverted\"\n",
    "]\n",
    "\n",
    "# Memory management settings\n",
    "CHUNK_SIZE = 10000          # Process data in chunks for large datasets\n",
    "MAX_MEMORY_GB = 8           # Maximum memory usage threshold\n",
    "ENABLE_MEMORY_OPTIMIZATION = True  # Enable memory-saving features\n",
    "\n",
    "# Imports\n",
    "import json, warnings, joblib\n",
    "# Optional psutil import for memory monitoring\n",
    "try:\n",
    "    import psutil\n",
    "    HAVE_PSUTIL = True\n",
    "except Exception:\n",
    "    HAVE_PSUTIL = False\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Optional: XGBoost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAVE_XGB = True\n",
    "except Exception:\n",
    "    HAVE_XGB = False\n",
    "    warnings.warn(\"XGBoost not available; skipping XGBoost.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f45a89",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Functions\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def check_memory_usage():\n",
    "    \"\"\"Monitor memory usage and warn if approaching limits.\"\"\"\n",
    "    if not (ENABLE_MEMORY_OPTIMIZATION and HAVE_PSUTIL):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        memory_gb = memory_info.used / (1024**3)\n",
    "        memory_percent = memory_info.percent\n",
    "        \n",
    "        if memory_gb > MAX_MEMORY_GB * 0.8:\n",
    "            logger.warning(f\"High memory usage: {memory_gb:.1f}GB / {MAX_MEMORY_GB}GB limit ({memory_percent:.1f}%)\")\n",
    "            return True\n",
    "        elif memory_percent > 85:\n",
    "            logger.warning(f\"System memory usage high: {memory_percent:.1f}% ({memory_gb:.1f}GB)\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Memory monitoring failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_multisheet_xlsx(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load multi-sheet Excel file, adding site_id from sheet names.\"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Excel file not found: {path}\")\n",
    "    \n",
    "    # Check memory before large operation\n",
    "    check_memory_usage()\n",
    "    \n",
    "    try:\n",
    "        book = pd.read_excel(path, sheet_name=None)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to read Excel file {path}: {e}\")\n",
    "    \n",
    "    frames = []\n",
    "    for sheet_name, df in book.items():\n",
    "        if df.empty:\n",
    "            logger.warning(f\"Empty sheet '{sheet_name}' skipped\")\n",
    "            continue\n",
    "        df = df.copy()\n",
    "        df[\"site_id\"] = sheet_name\n",
    "        frames.append(df)\n",
    "        logger.info(f\"Loaded sheet '{sheet_name}' with {len(df)} rows\")\n",
    "    \n",
    "    if not frames:\n",
    "        raise ValueError(\"No non-empty sheets found in Excel file.\")\n",
    "    \n",
    "    result = pd.concat(frames, ignore_index=True)\n",
    "    logger.info(f\"Combined {len(frames)} sheets into {len(result)} total rows\")\n",
    "    \n",
    "    # Check memory after operation\n",
    "    check_memory_usage()\n",
    "    return result\n",
    "\n",
    "def load_google_sheet_csv(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV from Google Sheets URL.\"\"\"\n",
    "    if not url:\n",
    "        raise ValueError(\"SHEET_CSV_URL is empty.\")\n",
    "    \n",
    "    logger.info(f\"Loading data from Google Sheets URL\")\n",
    "    check_memory_usage()\n",
    "    \n",
    "    try:\n",
    "        return pd.read_csv(url)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Standard CSV read failed: {e}. Trying with error handling...\")\n",
    "        try:\n",
    "            return pd.read_csv(url, engine=\"python\", on_bad_lines=\"skip\")\n",
    "        except Exception as e2:\n",
    "            raise ValueError(f\"Failed to load Google Sheets CSV: {e2}\")\n",
    "\n",
    "def load_local_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load local CSV file.\"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Local CSV not found: {p}\")\n",
    "    \n",
    "    check_memory_usage()\n",
    "    try:\n",
    "        result = pd.read_csv(p)\n",
    "        logger.info(f\"Loaded local CSV with {len(result)} rows\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to read CSV file {p}: {e}\")\n",
    "\n",
    "def validate_dataframe(df: pd.DataFrame, min_rows: int = 10) -> None:\n",
    "    \"\"\"Validate basic dataframe properties.\"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Dataframe is empty\")\n",
    "    if len(df) < min_rows:\n",
    "        raise ValueError(f\"Dataframe has only {len(df)} rows, minimum {min_rows} required\")\n",
    "    \n",
    "    # Check for completely empty columns\n",
    "    empty_cols = df.columns[df.isnull().all()].tolist()\n",
    "    if empty_cols:\n",
    "        logger.warning(f\"Completely empty columns found: {empty_cols}\")\n",
    "\n",
    "def unify_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize column names using aliases.\"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Cannot unify columns of empty dataframe\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    original_cols = df.columns.tolist()\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    \n",
    "    alias = {\n",
    "        \"Date\": \"timestamp\",\n",
    "        \"Soil Moisture %\": \"soil_moisture_pct\",\n",
    "        \"Soil Moisture (%)\": \"soil_moisture_pct\",\n",
    "        \"Humidity %\": \"humidity_pct\",\n",
    "        \"Humidity (%)\": \"humidity_pct\",\n",
    "        \"Temperature °C\": \"temperature_c\",\n",
    "        \"Temperature (°C)\": \"temperature_c\",\n",
    "        \"Light Intensity\": \"light_adc\",\n",
    "        \"Site\": \"site_id\",\n",
    "        \"site\": \"site_id\",\n",
    "        \"location\": \"site_id\",\n",
    "    }\n",
    "    \n",
    "    renamed_count = 0\n",
    "    for old, new in alias.items():\n",
    "        if old in df.columns and new not in df.columns:\n",
    "            df[new] = df[old]\n",
    "            renamed_count += 1\n",
    "    \n",
    "    logger.info(f\"Renamed {renamed_count} columns using aliases\")\n",
    "    \n",
    "    # Handle timestamps with better error handling\n",
    "    timestamp_cols = [\"timestamp\", \"Date\"]\n",
    "    timestamp_col = None\n",
    "    \n",
    "    for col in timestamp_cols:\n",
    "        if col in df.columns:\n",
    "            timestamp_col = col\n",
    "            break\n",
    "    \n",
    "    if timestamp_col:\n",
    "        try:\n",
    "            ts = pd.to_datetime(df[timestamp_col], errors=\"coerce\", utc=True)\n",
    "            if ts.isna().mean() > 0.5:\n",
    "                logger.warning(\"Many timestamp parsing failures, trying without UTC\")\n",
    "                ts = pd.to_datetime(df[timestamp_col], errors=\"coerce\")\n",
    "            \n",
    "            failed_parses = ts.isna().sum()\n",
    "            if failed_parses > 0:\n",
    "                logger.warning(f\"{failed_parses} timestamps failed to parse\")\n",
    "            \n",
    "            df[\"timestamp\"] = ts\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Timestamp parsing failed: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        logger.warning(\"No timestamp column found in data\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def choose_light_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Select the appropriate light column, inverting if needed, and standardize to 'light_adc' with normalized version.\"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Cannot choose light column from empty dataframe\")\n",
    "    \n",
    "    # Check for pre-inverted columns first\n",
    "    for c in LIGHT_HEADERS_INVERTED:\n",
    "        if c in df.columns:\n",
    "            non_null_count = df[c].notna().sum()\n",
    "            if non_null_count > 0:\n",
    "                df[\"light_adc\"] = df[c]  # Rename pre-inverted column to 'light_adc'\n",
    "                logger.info(f\"Using pre-inverted light column '{c}' with {non_null_count} valid values\")\n",
    "                \n",
    "                # Always provide a normalized light feature in [0,1] with clamping\n",
    "                df[\"light_norm\"] = pd.to_numeric(df[\"light_adc\"], errors=\"coerce\") / float(LIGHT_ADC_MAX)\n",
    "                df[\"light_norm\"] = df[\"light_norm\"].clip(lower=0.0, upper=1.0)  # Clamp to [0,1]\n",
    "                \n",
    "                # Log any clamped values\n",
    "                original_norm = pd.to_numeric(df[\"light_adc\"], errors=\"coerce\") / float(LIGHT_ADC_MAX)\n",
    "                clamped_low = (original_norm < 0.0).sum()\n",
    "                clamped_high = (original_norm > 1.0).sum()\n",
    "                if clamped_low > 0 or clamped_high > 0:\n",
    "                    logger.warning(f\"Clamped light values - Low: {clamped_low}, High: {clamped_high}\")\n",
    "                \n",
    "                logger.info(f\"Created normalized light feature 'light_norm' (0-1 scale, clamped)\")\n",
    "                \n",
    "                return \"light_adc\"\n",
    "    \n",
    "    # Check for raw ADC column that needs inversion\n",
    "    if \"light_adc\" in df.columns:\n",
    "        try:\n",
    "            original_values = pd.to_numeric(df[\"light_adc\"], errors=\"coerce\")\n",
    "            non_null_count = original_values.notna().sum()\n",
    "            \n",
    "            if non_null_count == 0:\n",
    "                raise ValueError(\"light_adc column contains no valid numeric data\")\n",
    "            \n",
    "            # Validate ADC range\n",
    "            min_val, max_val = original_values.min(), original_values.max()\n",
    "            if max_val > LIGHT_ADC_MAX:\n",
    "                logger.warning(f\"Light ADC values exceed expected max {LIGHT_ADC_MAX}: max={max_val}\")\n",
    "            \n",
    "            df[\"light_adc\"] = LIGHT_ADC_MAX - original_values  # Invert raw ADC\n",
    "            logger.info(f\"Inverted raw light_adc column (range: {min_val}-{max_val})\")\n",
    "            \n",
    "            # Always provide a normalized light feature in [0,1] with clamping\n",
    "            df[\"light_norm\"] = pd.to_numeric(df[\"light_adc\"], errors=\"coerce\") / float(LIGHT_ADC_MAX)\n",
    "            df[\"light_norm\"] = df[\"light_norm\"].clip(lower=0.0, upper=1.0)  # Clamp to [0,1]\n",
    "            \n",
    "            # Log any clamped values  \n",
    "            original_norm = pd.to_numeric(df[\"light_adc\"], errors=\"coerce\") / float(LIGHT_ADC_MAX)\n",
    "            clamped_low = (original_norm < 0.0).sum()\n",
    "            clamped_high = (original_norm > 1.0).sum()\n",
    "            if clamped_low > 0 or clamped_high > 0:\n",
    "                logger.warning(f\"Clamped light values - Low: {clamped_low}, High: {clamped_high}\")\n",
    "            \n",
    "            logger.info(f\"Created normalized light feature 'light_norm' (0-1 scale, clamped)\")\n",
    "            \n",
    "            return \"light_adc\"\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to process light_adc column: {e}\")\n",
    "    \n",
    "    available_cols = [c for c in df.columns if 'light' in c.lower()]\n",
    "    raise ValueError(f\"No light column found. Expected one of {LIGHT_HEADERS_INVERTED} \"\n",
    "                     f\"or raw 'light_adc'. Available light-related columns: {available_cols}\")\n",
    "\n",
    "# Load data based on config with comprehensive error handling\n",
    "try:\n",
    "    if MULTISHEET_XLSX:\n",
    "        logger.info(f\"Loading multi-sheet Excel: {MULTISHEET_XLSX}\")\n",
    "        raw = load_multisheet_xlsx(MULTISHEET_XLSX)\n",
    "    elif SHEET_CSV_URL:\n",
    "        logger.info(\"Loading from Google Sheets CSV URL\")\n",
    "        raw = load_google_sheet_csv(SHEET_CSV_URL)\n",
    "    else:\n",
    "        logger.info(f\"Loading local CSV: {LOCAL_CSV}\")\n",
    "        raw = load_local_csv(LOCAL_CSV)\n",
    "    \n",
    "    # Validate loaded data\n",
    "    validate_dataframe(raw, min_rows=50)  # Require at least 50 rows for meaningful ML\n",
    "    \n",
    "    # Process columns\n",
    "    raw = unify_columns(raw)\n",
    "    light_col = choose_light_column(raw)\n",
    "    \n",
    "    # Check for required columns\n",
    "    required = [\"timestamp\", \"soil_moisture_pct\", \"temperature_c\", \"humidity_pct\"]\n",
    "    missing = [c for c in required if c not in raw.columns]\n",
    "    \n",
    "    if missing:\n",
    "        logger.error(f\"Missing required columns: {missing}\")\n",
    "        available = list(raw.columns)\n",
    "        logger.info(f\"Available columns: {available}\")\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    \n",
    "    # Validate data ranges\n",
    "    for col in [\"soil_moisture_pct\", \"humidity_pct\"]:\n",
    "        if col in raw.columns:\n",
    "            valid_data = pd.to_numeric(raw[col], errors=\"coerce\")\n",
    "            out_of_range = ((valid_data < 0) | (valid_data > 100)).sum()\n",
    "            if out_of_range > 0:\n",
    "                logger.warning(f\"{col}: {out_of_range} values outside 0-100% range\")\n",
    "    \n",
    "    logger.info(f\"Successfully loaded and validated {len(raw)} rows\")\n",
    "    print(f\"Data loaded successfully: {len(raw)} rows, {len(raw.columns)} columns\")\n",
    "    raw.tail(3)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Data loading failed: {e}\")\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data as CSV\n",
    "raw.to_csv(\"siams_cleaned.csv\", index=False)\n",
    "print(\"Saved cleaned data to: siams_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary per Site\n",
    "summary = raw.groupby(\"site_id\")[\"timestamp\"].agg(n_records=\"count\", start=\"min\", end=\"max\").reset_index()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c636f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary as CSV\n",
    "summary.to_csv(\"siams_summary.csv\", index=False)\n",
    "print(\"Saved summary to: siams_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cde1bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EDA Plots (Matplotlib only)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [\n",
    "    (\"temperature_c\",\"Temperature (°C)\"),\n",
    "    (\"humidity_pct\",\"Humidity (%)\"),\n",
    "    (\"soil_moisture_pct\",\"Soil Moisture (%)\"),\n",
    "    (\"light_norm\",\"Light Intensity (Normalized 0-1)\")  # Updated to use normalized light\n",
    "]\n",
    "\n",
    "plot_dir = Path(\"plots\")\n",
    "plot_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "sites = sorted(raw[\"site_id\"].dropna().unique())\n",
    "\n",
    "for col, label in metrics:\n",
    "    if col not in raw.columns:\n",
    "        continue\n",
    "    plt.figure()\n",
    "    data = [raw.loc[raw[\"site_id\"]==s, col].dropna() for s in sites]\n",
    "    plt.boxplot(data, tick_labels=sites, showfliers=False)\n",
    "    plt.title(f\"{label} by Site — Boxplot\")\n",
    "    plt.xlabel(\"Site\"); plt.ylabel(label)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_dir / f\"boxplot_{col}.png\", dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "for col, label in metrics:\n",
    "    if col not in raw.columns:\n",
    "        continue\n",
    "    for s in sites:\n",
    "        subset = raw.loc[raw[\"site_id\"]==s, col].dropna()\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        plt.figure()\n",
    "        plt.hist(subset, bins=30)\n",
    "        plt.title(f\"{label} — Histogram ({s})\")\n",
    "        plt.xlabel(label); plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_dir / f\"hist_{col}_{s}.png\", dpi=160)\n",
    "        plt.show()\n",
    "\n",
    "print(\"Saved plots to:\", plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643079ae",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ea651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Functions\n",
    "def add_calendar_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add time-based features: hour, dayofweek, month (Africa/Lagos timezone).\n",
    "    \n",
    "    Handles both timezone-aware and timezone-naive timestamps safely.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    ts = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    \n",
    "    # If ts is tz-naive, localize to UTC first, then convert\n",
    "    if ts.dt.tz is None:\n",
    "        ts = ts.dt.tz_localize(\"UTC\", nonexistent=\"NaT\", ambiguous=\"NaT\")\n",
    "    \n",
    "    ts_local = ts.dt.tz_convert(\"Africa/Lagos\")\n",
    "    df[\"hour\"] = ts_local.dt.hour\n",
    "    df[\"dayofweek\"] = ts_local.dt.dayofweek\n",
    "    df[\"month\"] = ts_local.dt.month\n",
    "    return df\n",
    "\n",
    "def add_lags_and_rolls(df: pd.DataFrame, lags=(1,2,3), roll_window=3) -> pd.DataFrame:\n",
    "    \"\"\"Add lagged and rolling features for soil moisture.\"\"\"\n",
    "    if \"site_id\" in df.columns:\n",
    "        df = df.sort_values([\"site_id\",\"timestamp\"]).copy()\n",
    "        def _apply(g):\n",
    "            for L in lags:\n",
    "                g[f\"soil_moisture_lag{L}\"] = g[\"soil_moisture_pct\"].shift(L)\n",
    "            if roll_window and roll_window > 1:\n",
    "                past = g[\"soil_moisture_pct\"].shift(1)\n",
    "                g[f\"soil_moisture_roll{roll_window}\"] = past.rolling(roll_window, min_periods=1).mean()\n",
    "            return g\n",
    "        df = df.groupby(\"site_id\", group_keys=False).apply(_apply)\n",
    "    else:\n",
    "        df = df.sort_values([\"timestamp\"]).copy()\n",
    "        for L in lags:\n",
    "            df[f\"soil_moisture_lag{L}\"] = df[\"soil_moisture_pct\"].shift(L)\n",
    "        if roll_window and roll_window > 1:\n",
    "            past = df[\"soil_moisture_pct\"].shift(1)\n",
    "            df[f\"soil_moisture_roll{roll_window}\"] = past.rolling(roll_window, min_periods=1).mean()\n",
    "    return df\n",
    "\n",
    "def add_plant_health_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add comprehensive plant health and environmental stress metrics.\"\"\"\n",
    "    logger.info(\"Calculating plant health metrics...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert to numeric for calculations\n",
    "    T = pd.to_numeric(df.get(\"temperature_c\"), errors=\"coerce\")\n",
    "    RH = pd.to_numeric(df.get(\"humidity_pct\"), errors=\"coerce\")\n",
    "    SM = pd.to_numeric(df.get(\"soil_moisture_pct\"), errors=\"coerce\")\n",
    "    \n",
    "    # Validate input ranges and log warnings\n",
    "    invalid_temp = ((T < -50) | (T > 70)).sum()\n",
    "    invalid_humidity = ((RH < 0) | (RH > 100)).sum()\n",
    "    invalid_moisture = ((SM < 0) | (SM > 100)).sum()\n",
    "    \n",
    "    if invalid_temp > 0:\n",
    "        logger.warning(f\"{invalid_temp} temperature readings outside realistic range (-50°C to 70°C)\")\n",
    "    if invalid_humidity > 0:\n",
    "        logger.warning(f\"{invalid_humidity} humidity readings outside 0-100% range\")\n",
    "    if invalid_moisture > 0:\n",
    "        logger.warning(f\"{invalid_moisture} soil moisture readings outside 0-100% range\")\n",
    "    \n",
    "    # Check for sensor data availability\n",
    "    temp_available = T.notna().sum()\n",
    "    humidity_available = RH.notna().sum()\n",
    "    moisture_available = SM.notna().sum()\n",
    "    \n",
    "    if temp_available == 0:\n",
    "        logger.error(\"No valid temperature data available for health metrics\")\n",
    "        raise ValueError(\"Temperature data required for health metrics calculation\")\n",
    "    if humidity_available == 0:\n",
    "        logger.error(\"No valid humidity data available for health metrics\")\n",
    "        raise ValueError(\"Humidity data required for health metrics calculation\")\n",
    "    if moisture_available == 0:\n",
    "        logger.error(\"No valid soil moisture data available for health metrics\")\n",
    "        raise ValueError(\"Soil moisture data required for health metrics calculation\")\n",
    "    \n",
    "    logger.info(f\"Health metrics input validation: {temp_available} temp, {humidity_available} humidity, {moisture_available} moisture readings\")\n",
    "    \n",
    "    # Check memory usage before intensive calculations\n",
    "    if check_memory_usage():\n",
    "        logger.info(\"High memory usage detected, optimizing calculations...\")\n",
    "    \n",
    "    # Vapor Pressure Deficit (VPD) in kPa - critical for plant stress assessment\n",
    "    try:\n",
    "        es = 0.6108 * np.exp((17.27 * T) / (T + 237.3))  # Saturation vapor pressure\n",
    "        ea = es * (RH / 100.0)  # Actual vapor pressure\n",
    "        df[\"vpd_kpa\"] = (es - ea).clip(lower=0)\n",
    "        \n",
    "        # Validate VPD range (typically 0-6 kPa)\n",
    "        extreme_vpd = (df[\"vpd_kpa\"] > 6.0).sum()\n",
    "        if extreme_vpd > 0:\n",
    "            logger.warning(f\"{extreme_vpd} VPD values exceed 6.0 kPa (extreme conditions)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"VPD calculation failed: {e}\")\n",
    "        df[\"vpd_kpa\"] = np.nan\n",
    "    \n",
    "    # Dew point calculation (°C) - important for disease risk\n",
    "    try:\n",
    "        a, b = 17.27, 237.3\n",
    "        alpha = (a * T) / (b + T) + np.log((RH / 100.0).clip(lower=1e-6))\n",
    "        df[\"dew_point_c\"] = (b * alpha) / (a - alpha)\n",
    "        \n",
    "        # Validate dew point (should be <= temperature)\n",
    "        invalid_dewpoint = (df[\"dew_point_c\"] > T).sum()\n",
    "        if invalid_dewpoint > 0:\n",
    "            logger.warning(f\"{invalid_dewpoint} dew point values exceed temperature (calculation error)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Dew point calculation failed: {e}\")\n",
    "        df[\"dew_point_c\"] = np.nan\n",
    "    \n",
    "    # Environmental stress flags with validation\n",
    "    df[\"heat_flag\"] = (T >= HEAT_STRESS_TEMP).astype(int)   # Heat stress indicator\n",
    "    df[\"frost_flag\"] = (T <= FROST_RISK_TEMP).astype(int)   # Frost risk indicator\n",
    "    \n",
    "    heat_events = df[\"heat_flag\"].sum()\n",
    "    frost_events = df[\"frost_flag\"].sum()\n",
    "    if heat_events > 0:\n",
    "        logger.info(f\"Detected {heat_events} heat stress events (≥{HEAT_STRESS_TEMP}°C)\")\n",
    "    if frost_events > 0:\n",
    "        logger.info(f\"Detected {frost_events} frost risk events (≤{FROST_RISK_TEMP}°C)\")\n",
    "    \n",
    "    # Soil moisture pattern analysis (rolling averages per site)\n",
    "    try:\n",
    "        if \"site_id\" in df.columns:\n",
    "            df[\"moist_roll6\"] = df.groupby(\"site_id\")[\"soil_moisture_pct\"].transform(\n",
    "                lambda s: pd.to_numeric(s, errors=\"coerce\").rolling(MOISTURE_PATTERN_WINDOW, min_periods=3).mean()\n",
    "            )\n",
    "        else:\n",
    "            df[\"moist_roll6\"] = pd.to_numeric(df[\"soil_moisture_pct\"], errors=\"coerce\").rolling(MOISTURE_PATTERN_WINDOW, min_periods=3).mean()\n",
    "        \n",
    "        df[\"waterlog_flag\"] = (df[\"moist_roll6\"] >= WATERLOG_THRESHOLD).astype(int)  # Waterlogging risk\n",
    "        df[\"dryspell_flag\"] = (df[\"moist_roll6\"] <= DRYSPELL_THRESHOLD).astype(int)  # Drought stress\n",
    "        \n",
    "        waterlog_events = df[\"waterlog_flag\"].sum()\n",
    "        dryspell_events = df[\"dryspell_flag\"].sum()\n",
    "        if waterlog_events > 0:\n",
    "            logger.info(f\"Detected {waterlog_events} waterlogging risk events (≥{WATERLOG_THRESHOLD}%)\")\n",
    "        if dryspell_events > 0:\n",
    "            logger.info(f\"Detected {dryspell_events} dry spell events (≤{DRYSPELL_THRESHOLD}%)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Moisture pattern analysis failed: {e}\")\n",
    "        df[\"moist_roll6\"] = np.nan\n",
    "        df[\"waterlog_flag\"] = 0\n",
    "        df[\"dryspell_flag\"] = 0\n",
    "    \n",
    "    # Disease risk assessment (0-100 scale) with validation\n",
    "    try:\n",
    "        # High risk conditions: high humidity (>85%), optimal temp (18-28°C), low VPD (<0.6 kPa)\n",
    "        temp_optimal = ((T >= 18) & (T <= 28)).astype(int)\n",
    "        vpd_conducive = (df[\"vpd_kpa\"] <= VPD_DISEASE_THRESHOLD).astype(int)\n",
    "        humidity_high = (RH >= 85).astype(int)\n",
    "        \n",
    "        # Weighted disease risk score\n",
    "        df[\"disease_risk\"] = (\n",
    "            (0.5 * humidity_high + 0.3 * temp_optimal + 0.2 * vpd_conducive) / (0.5 + 0.3 + 0.2) * 100\n",
    "        ).round(1)\n",
    "        \n",
    "        high_risk_periods = (df[\"disease_risk\"] > 75).sum()\n",
    "        if high_risk_periods > 0:\n",
    "            logger.info(f\"Detected {high_risk_periods} high disease risk periods (>75% score)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Disease risk calculation failed: {e}\")\n",
    "        df[\"disease_risk\"] = 0\n",
    "    \n",
    "    # Sensor health monitoring - detect \"stuck\" sensors (low variability)\n",
    "    sensor_columns = [\"soil_moisture_pct\", \"temperature_c\", \"humidity_pct\", \"light_adc\"]\n",
    "    sensor_issues_detected = 0\n",
    "    \n",
    "    try:\n",
    "        if \"site_id\" in df.columns:\n",
    "            # Per-site sensor health analysis\n",
    "            for col in sensor_columns:\n",
    "                if col in df.columns:\n",
    "                    rolling_std = df.groupby(\"site_id\")[col].transform(\n",
    "                        lambda s: pd.to_numeric(s, errors=\"coerce\").rolling(SENSOR_STABILITY_WINDOW, min_periods=6).std()\n",
    "                    )\n",
    "                    df[f\"{col}_stuck\"] = (rolling_std.fillna(0) == 0).astype(int)\n",
    "                    stuck_count = df[f\"{col}_stuck\"].sum()\n",
    "                    if stuck_count > 0:\n",
    "                        sensor_issues_detected += stuck_count\n",
    "                        logger.warning(f\"Sensor {col}: {stuck_count} potential stuck readings detected\")\n",
    "        else:\n",
    "            # Global sensor health analysis\n",
    "            for col in sensor_columns:\n",
    "                if col in df.columns:\n",
    "                    rolling_std = pd.to_numeric(df[col], errors=\"coerce\").rolling(SENSOR_STABILITY_WINDOW, min_periods=6).std()\n",
    "                    df[f\"{col}_stuck\"] = (rolling_std.fillna(0) == 0).astype(int)\n",
    "                    stuck_count = df[f\"{col}_stuck\"].sum()\n",
    "                    if stuck_count > 0:\n",
    "                        sensor_issues_detected += stuck_count\n",
    "                        logger.warning(f\"Sensor {col}: {stuck_count} potential stuck readings detected\")\n",
    "        \n",
    "        # Overall sensor issue flag\n",
    "        stuck_columns = [f\"{c}_stuck\" for c in sensor_columns if f\"{c}_stuck\" in df.columns]\n",
    "        if stuck_columns:\n",
    "            df[\"sensor_issue_flag\"] = df[stuck_columns].any(axis=1).astype(int)\n",
    "            total_sensor_issues = df[\"sensor_issue_flag\"].sum()\n",
    "            if total_sensor_issues > 0:\n",
    "                logger.warning(f\"Total sensor issues detected: {total_sensor_issues} readings flagged\")\n",
    "        else:\n",
    "            df[\"sensor_issue_flag\"] = 0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Sensor health monitoring failed: {e}\")\n",
    "        df[\"sensor_issue_flag\"] = 0\n",
    "    \n",
    "    # Final validation and summary\n",
    "    health_metrics_created = [\"vpd_kpa\", \"dew_point_c\", \"heat_flag\", \"frost_flag\", \n",
    "                             \"waterlog_flag\", \"dryspell_flag\", \"disease_risk\", \"sensor_issue_flag\"]\n",
    "    \n",
    "    successful_metrics = [col for col in health_metrics_created if col in df.columns and df[col].notna().any()]\n",
    "    failed_metrics = [col for col in health_metrics_created if col not in df.columns or df[col].isna().all()]\n",
    "    \n",
    "    if failed_metrics:\n",
    "        logger.warning(f\"Failed to create health metrics: {failed_metrics}\")\n",
    "    \n",
    "    logger.info(f\"Plant health metrics calculation completed: {len(successful_metrics)}/{len(health_metrics_created)} metrics successful\")\n",
    "    return df\n",
    "\n",
    "def one_hot_encode_sites(df: pd.DataFrame, known_sites) -> pd.DataFrame:\n",
    "    \"\"\"One-hot encode site_id with fixed order.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"site_id\" not in df.columns:\n",
    "        df[\"site_id\"] = known_sites[0]  # fallback\n",
    "    dummies = pd.get_dummies(df[\"site_id\"], prefix=\"site_id\")\n",
    "    desired = [f\"site_id_{s}\" for s in known_sites]\n",
    "    for col in desired:\n",
    "        if col not in dummies.columns:\n",
    "            dummies[col] = 0\n",
    "    dummies = dummies.reindex(columns=desired, fill_value=0)\n",
    "    return pd.concat([df.drop(columns=[\"site_id\"], errors=\"ignore\"), dummies], axis=1)\n",
    "\n",
    "# Apply comprehensive feature engineering\n",
    "logger.info(\"Starting comprehensive feature engineering...\")\n",
    "features = raw.copy()\n",
    "features = add_calendar_features(features)\n",
    "features = add_lags_and_rolls(features, lags=LAG_WINDOWS, roll_window=ROLLING_WINDOW)\n",
    "features = add_plant_health_metrics(features)  # Add health metrics\n",
    "features = one_hot_encode_sites(features, KNOWN_SITES)\n",
    "\n",
    "# Define comprehensive feature columns including health metrics\n",
    "site_cols = [f\"site_id_{s}\" for s in KNOWN_SITES]\n",
    "HEALTH_COLS_TRAIN = [\"vpd_kpa\", \"dew_point_c\", \"heat_flag\", \"frost_flag\", \"disease_risk\"]  # safe (no target leakage)\n",
    "HEALTH_EXPORT_COLS = [\n",
    "    \"timestamp\", \"site_id\", \"vpd_kpa\", \"dew_point_c\", \"heat_flag\", \"frost_flag\",\n",
    "    \"waterlog_flag\", \"dryspell_flag\", \"disease_risk\", \"sensor_issue_flag\"\n",
    "]  # for app display\n",
    "\n",
    "feat_cols = [\n",
    "    # Core environmental features\n",
    "    \"temperature_c\", \"humidity_pct\", \"light_norm\",   # <- Changed from light_adc to light_norm\n",
    "    # Temporal features\n",
    "    \"hour\", \"dayofweek\", \"month\",\n",
    "    # Soil moisture patterns\n",
    "    *[f\"soil_moisture_lag{L}\" for L in LAG_WINDOWS],\n",
    "    f\"soil_moisture_roll{ROLLING_WINDOW}\",\n",
    "    # Plant health and stress indicators (training-safe only)\n",
    "    *HEALTH_COLS_TRAIN,\n",
    "    # Site identification\n",
    "    *site_cols\n",
    "]\n",
    "\n",
    "# Ensure all features exist\n",
    "for c in feat_cols:\n",
    "    if c not in features.columns:\n",
    "        features[c] = np.nan\n",
    "\n",
    "# Prepare X and y but keep original indices for splitting\n",
    "X_full = features[feat_cols].astype(float)\n",
    "y_full = pd.to_numeric(features[\"soil_moisture_pct\"], errors=\"coerce\")\n",
    "\n",
    "# Apply mask to identify valid rows\n",
    "mask = X_full.notna().all(axis=1) & y_full.notna()\n",
    "\n",
    "# Filter to get clean data, preserving original indices\n",
    "X = X_full[mask]\n",
    "y = y_full[mask]\n",
    "\n",
    "print(\"Final feature order used for training/inference:\")\n",
    "print(feat_cols)\n",
    "print(f\"Total features: {len(feat_cols)} (including {len(HEALTH_COLS_TRAIN)} health metrics)\")\n",
    "print(\"Rows available:\", len(X))\n",
    "X.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features as CSV\n",
    "features.to_csv(\"siams_features.csv\", index=False)\n",
    "print(\"Saved features to: siams_features.csv\")\n",
    "\n",
    "# Export health metrics for the Streamlit app\n",
    "# Note: HEALTH_EXPORT_COLS is already defined earlier near feature engineering\n",
    "\n",
    "def reconstruct_site_id(df, known_sites=KNOWN_SITES):\n",
    "    \"\"\"Reconstruct site_id from one-hot encoded columns.\"\"\"\n",
    "    scols = [f\"site_id_{s}\" for s in known_sites]\n",
    "    present = [c for c in scols if c in df.columns]\n",
    "    if not present:\n",
    "        return pd.Series(\"unknown\", index=df.index)\n",
    "    return df[present].idxmax(axis=1).str.replace(\"site_id_\", \"\", regex=False)\n",
    "\n",
    "# Reconstruct site_id from one-hot columns before export (since it's dropped during one-hot encoding)\n",
    "if \"site_id\" not in features.columns:\n",
    "    features[\"site_id\"] = reconstruct_site_id(features)\n",
    "\n",
    "# Ensure all columns exist before saving\n",
    "available_health_cols = [col for col in HEALTH_EXPORT_COLS if col in features.columns]\n",
    "missing_cols = set(HEALTH_EXPORT_COLS) - set(available_health_cols)\n",
    "if missing_cols:\n",
    "    logger.warning(f\"Missing health export columns: {missing_cols}\")\n",
    "\n",
    "health_export = features[available_health_cols].copy()\n",
    "health_export.to_csv(\"siams_health_metrics.csv\", index=False)\n",
    "logger.info(f\"Exported health metrics with {len(available_health_cols)} columns to siams_health_metrics.csv\")\n",
    "\n",
    "# Display health metrics summary\n",
    "print(\"\\nPlant Health Metrics Summary:\")\n",
    "health_summary_cols = [col for col in HEALTH_ALL if col in features.columns]\n",
    "if health_summary_cols:\n",
    "    health_summary = features[health_summary_cols].describe()\n",
    "    print(health_summary)\n",
    "else:\n",
    "    print(\"No health metrics available for summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b26f6e",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b98f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "def validate_model_inputs(X: pd.DataFrame, y: pd.Series, min_samples: int = 100) -> None:\n",
    "    \"\"\"Validate inputs for model training.\"\"\"\n",
    "    if X.empty or y.empty:\n",
    "        raise ValueError(\"Empty training data provided\")\n",
    "    \n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"Feature matrix ({len(X)}) and target ({len(y)}) length mismatch\")\n",
    "    \n",
    "    if len(X) < min_samples:\n",
    "        raise ValueError(f\"Insufficient data: {len(X)} samples, minimum {min_samples} required\")\n",
    "    \n",
    "    # Check for infinite or NaN values\n",
    "    if X.isnull().any().any():\n",
    "        null_cols = X.columns[X.isnull().any()].tolist()\n",
    "        raise ValueError(f\"NaN values found in features: {null_cols}\")\n",
    "    \n",
    "    if y.isnull().any():\n",
    "        raise ValueError(\"NaN values found in target variable\")\n",
    "    \n",
    "    if np.isinf(X.values).any():\n",
    "        raise ValueError(\"Infinite values found in feature matrix\")\n",
    "    \n",
    "    if np.isinf(y.values).any():\n",
    "        raise ValueError(\"Infinite values found in target variable\")\n",
    "    \n",
    "    logger.info(f\"Model input validation passed: {len(X)} samples, {len(X.columns)} features\")\n",
    "\n",
    "def infer_site_label(df_feat: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Infer site labels from one-hot encoded columns.\"\"\"\n",
    "    scols = [f\"site_id_{s}\" for s in KNOWN_SITES]\n",
    "    \n",
    "    if not all(c in df_feat.columns for c in scols):\n",
    "        missing_sites = [c for c in scols if c not in df_feat.columns]\n",
    "        logger.warning(f\"Missing site columns: {missing_sites}\")\n",
    "        return pd.Series([\"unknown\"] * len(df_feat), index=df_feat.index)\n",
    "    \n",
    "    # Check for rows with no site assigned (all zeros)\n",
    "    site_matrix = df_feat[scols]\n",
    "    row_sums = site_matrix.sum(axis=1)\n",
    "    unassigned = (row_sums == 0).sum()\n",
    "    \n",
    "    if unassigned > 0:\n",
    "        logger.warning(f\"{unassigned} rows have no site assignment\")\n",
    "    \n",
    "    # Handle multiple site assignments (should not happen with proper one-hot encoding)\n",
    "    multiple_sites = (row_sums > 1).sum()\n",
    "    if multiple_sites > 0:\n",
    "        logger.warning(f\"{multiple_sites} rows have multiple site assignments\")\n",
    "    \n",
    "    lab = site_matrix.idxmax(axis=1).str.replace(\"site_id_\", \"\", regex=False)\n",
    "    return lab\n",
    "\n",
    "def time_aware_split_indices(df_feat: pd.DataFrame, test_fraction: float = TEST_FRACTION) -> tuple:\n",
    "    \"\"\"Perform time-aware train/test split per site.\"\"\"\n",
    "    if not 0 < test_fraction < 1:\n",
    "        raise ValueError(f\"test_fraction must be between 0 and 1, got {test_fraction}\")\n",
    "    \n",
    "    lab = infer_site_label(df_feat)\n",
    "    \n",
    "    # Get timestamps for the filtered dataframe indices\n",
    "    ts = pd.to_datetime(raw.loc[df_feat.index, \"timestamp\"], errors=\"coerce\")\n",
    "    \n",
    "    if ts.isnull().any():\n",
    "        null_timestamps = ts.isnull().sum()\n",
    "        logger.warning(f\"{null_timestamps} rows have invalid timestamps\")\n",
    "        # Remove rows with invalid timestamps\n",
    "        valid_mask = ts.notnull()\n",
    "        df_feat = df_feat[valid_mask]\n",
    "        lab = lab[valid_mask]\n",
    "        ts = ts[valid_mask]\n",
    "    \n",
    "    train_idx, test_idx = [], []\n",
    "    site_stats = {}\n",
    "    \n",
    "    for site in lab.unique():\n",
    "        if site == \"unknown\":\n",
    "            continue\n",
    "            \n",
    "        site_mask = (lab == site)\n",
    "        idx = df_feat.index[site_mask]\n",
    "        site_timestamps = ts.loc[idx]\n",
    "        \n",
    "        # TZ-safe time sort with fallback for older pandas versions\n",
    "        ts_utc = pd.to_datetime(site_timestamps, utc=True)\n",
    "        try:\n",
    "            ints = ts_utc.view(\"int64\").values\n",
    "        except Exception:\n",
    "            ints = ts_utc.astype(\"int64\").values\n",
    "        order = np.argsort(ints)\n",
    "        sorted_indices = idx[order]\n",
    "        \n",
    "        n = len(sorted_indices)\n",
    "        if n == 0:\n",
    "            continue\n",
    "            \n",
    "        k = int(round((1 - test_fraction) * n))\n",
    "        \n",
    "        # Ensure at least one sample in each split\n",
    "        k = max(1, min(k, n - 1))\n",
    "        \n",
    "        site_train = sorted_indices[:k]\n",
    "        site_test = sorted_indices[k:]\n",
    "        \n",
    "        train_idx.extend(site_train)\n",
    "        test_idx.extend(site_test)\n",
    "        \n",
    "        site_stats[site] = {\n",
    "            'total': n,\n",
    "            'train': len(site_train),\n",
    "            'test': len(site_test),\n",
    "            'train_end': site_timestamps.loc[site_train].max(),\n",
    "            'test_start': site_timestamps.loc[site_test].min()\n",
    "        }\n",
    "    \n",
    "    # Log split statistics\n",
    "    total_train, total_test = len(train_idx), len(test_idx)\n",
    "    actual_test_fraction = total_test / (total_train + total_test)\n",
    "    \n",
    "    logger.info(f\"Time-aware split: {total_train} train, {total_test} test \"\n",
    "                f\"({actual_test_fraction:.1%} test fraction)\")\n",
    "    \n",
    "    for site, stats in site_stats.items():\n",
    "        logger.info(f\"Site {site}: {stats['train']} train, {stats['test']} test\")\n",
    "    \n",
    "    return np.array(train_idx), np.array(test_idx)\n",
    "\n",
    "# Split data with validation - use filtered features dataframe\n",
    "try:\n",
    "    # Use the filtered features (those with valid data) for the split\n",
    "    features_filtered = features.loc[X.index]\n",
    "    train_idx, test_idx = time_aware_split_indices(features_filtered)\n",
    "    \n",
    "    # Validate splits\n",
    "    if len(train_idx) == 0 or len(test_idx) == 0:\n",
    "        raise ValueError(\"Empty train or test split\")\n",
    "    \n",
    "    # Check for overlap (should never happen)\n",
    "    overlap = set(train_idx) & set(test_idx)\n",
    "    if overlap:\n",
    "        raise ValueError(f\"Train/test overlap detected: {len(overlap)} samples\")\n",
    "    \n",
    "    # Ensure all indices exist in our filtered data\n",
    "    train_idx = [idx for idx in train_idx if idx in X.index]\n",
    "    test_idx = [idx for idx in test_idx if idx in X.index]\n",
    "    \n",
    "    # Final validation\n",
    "    if len(train_idx) < 50:\n",
    "        raise ValueError(f\"Insufficient training samples after filtering: {len(train_idx)}\")\n",
    "    if len(test_idx) < 10:\n",
    "        raise ValueError(f\"Insufficient test samples after filtering: {len(test_idx)}\")\n",
    "    \n",
    "    logger.info(f\"Final split - Train: {len(train_idx)}, Test: {len(test_idx)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Train/test split failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define models with better configuration\n",
    "models = {\n",
    "    \"DecisionTree\": DecisionTreeRegressor(\n",
    "        random_state=RANDOM_STATE,\n",
    "        min_samples_leaf=2,  # Prevent overfitting\n",
    "        min_samples_split=5   # Prevent overfitting\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        n_estimators=RF_N_ESTIMATORS, \n",
    "        max_depth=RF_MAX_DEPTH, \n",
    "        min_samples_leaf=RF_MIN_SAMPLES,\n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1,\n",
    "        oob_score=True  # Out-of-bag scoring for additional validation\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(\n",
    "        random_state=RANDOM_STATE,\n",
    "        validation_fraction=0.1,  # Use for early stopping\n",
    "        n_iter_no_change=10       # Early stopping patience\n",
    "    ),\n",
    "}\n",
    "\n",
    "if HAVE_XGB:\n",
    "    models[\"XGBoost\"] = XGBRegressor(\n",
    "        n_estimators=XGB_N_ESTIMATORS, \n",
    "        learning_rate=XGB_LEARNING_RATE, \n",
    "        max_depth=XGB_MAX_DEPTH,\n",
    "        subsample=0.9, \n",
    "        colsample_bytree=0.9, \n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1, \n",
    "        reg_lambda=1.0, \n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"rmse\"         # Explicit evaluation metric\n",
    "    )\n",
    "\n",
    "def evaluate(model, Xtr: pd.DataFrame, ytr: pd.Series, Xte: pd.DataFrame, yte: pd.Series) -> tuple:\n",
    "    \"\"\"Train model and compute metrics, clipping predictions.\"\"\"\n",
    "    # Validate inputs\n",
    "    validate_model_inputs(Xtr, ytr)\n",
    "    validate_model_inputs(Xte, yte, min_samples=10)  # Less strict for test set\n",
    "    \n",
    "    # Detect XGBoost using proper type checking\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        is_xgb = isinstance(model, xgb.XGBRegressor)\n",
    "    except Exception:\n",
    "        is_xgb = False\n",
    "\n",
    "    if is_xgb:\n",
    "        # XGBoost early stopping with validation split\n",
    "        val_size = max(10, int(0.1 * len(Xtr)))\n",
    "        model.fit(\n",
    "            Xtr.iloc[:-val_size], ytr.iloc[:-val_size],\n",
    "            eval_set=[(Xtr.iloc[-val_size:], ytr.iloc[-val_size:])],\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        model.fit(Xtr, ytr)\n",
    "    \n",
    "    # Make predictions and clip to realistic range\n",
    "    pred = model.predict(Xte)\n",
    "    pred_clipped = np.clip(pred, 0, 100)\n",
    "    \n",
    "    # Log clipping statistics\n",
    "    clipped_count = (pred != pred_clipped).sum()\n",
    "    if clipped_count > 0:\n",
    "        logger.info(f\"Clipped {clipped_count}/{len(pred)} predictions to 0-100% range\")\n",
    "    \n",
    "    # Calculate metrics - Fix for older scikit-learn versions\n",
    "    mse = mean_squared_error(yte, pred_clipped)\n",
    "    rmse = np.sqrt(mse)  # Calculate RMSE manually\n",
    "    mae = mean_absolute_error(yte, pred_clipped)\n",
    "    r2 = r2_score(yte, pred_clipped)\n",
    "    \n",
    "    return rmse, mae, r2, pred_clipped\n",
    "\n",
    "# Train and evaluate models with comprehensive error handling\n",
    "rows = []\n",
    "best = None\n",
    "model_errors = {}\n",
    "\n",
    "for name, mdl in models.items():\n",
    "    try:\n",
    "        logger.info(f\"Training {name} model...\")\n",
    "        rmse, mae, r2, _ = evaluate(mdl, X.loc[train_idx], y.loc[train_idx], \n",
    "                                    X.loc[test_idx], y.loc[test_idx])\n",
    "        \n",
    "        rows.append({\"Model\": name, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
    "        \n",
    "        # Track best model\n",
    "        if best is None or r2 > best[\"R2\"] or (r2 == best[\"R2\"] and rmse < best[\"RMSE\"]):\n",
    "            best = {\"Name\": name, \"Model\": mdl, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "        \n",
    "        logger.info(f\"{name}: RMSE={rmse:.3f}, MAE={mae:.3f}, R²={r2:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to train {name}: {e}\"\n",
    "        logger.error(error_msg)\n",
    "        model_errors[name] = str(e)\n",
    "        # Continue with other models\n",
    "\n",
    "# Enhanced error recovery with fallback baseline model\n",
    "if not rows:\n",
    "    logger.warning(\"All advanced models failed, attempting baseline model as fallback...\")\n",
    "    try:\n",
    "        # Fallback baseline model with minimal complexity\n",
    "        baseline_model = DecisionTreeRegressor(\n",
    "            max_depth=5, \n",
    "            min_samples_leaf=10, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Training baseline DecisionTree model...\")\n",
    "        rmse, mae, r2, _ = evaluate(baseline_model, X.loc[train_idx], y.loc[train_idx], \n",
    "                                   X.loc[test_idx], y.loc[test_idx])\n",
    "        \n",
    "        rows.append({\"Model\": \"Baseline_DecisionTree\", \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
    "        best = {\"Name\": \"Baseline_DecisionTree\", \"Model\": baseline_model, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "        \n",
    "        logger.info(f\"Baseline model successfully trained: RMSE={rmse:.3f}, MAE={mae:.3f}, R²={r2:.3f}\")\n",
    "        print(\"WARNING: All advanced models failed - using baseline DecisionTree model\")\n",
    "        \n",
    "    except Exception as baseline_error:\n",
    "        logger.error(f\"Even baseline model failed: {baseline_error}\")\n",
    "        print(\"\\nCRITICAL ERROR: All models (including baseline) failed to train!\")\n",
    "        print(\"Possible issues:\")\n",
    "        print(\"  • Data quality problems (too many missing values)\")\n",
    "        print(\"  • Insufficient training data\")\n",
    "        print(\"  • Feature engineering errors\")\n",
    "        print(\"  • Memory/resource constraints\")\n",
    "        print(\"\\nCheck the logs for detailed error information.\")\n",
    "        raise RuntimeError(\"All models failed to train. Check data quality and parameters.\")\n",
    "\n",
    "if not rows:  # Final check after baseline attempt\n",
    "    raise RuntimeError(\"No models successfully trained.\")\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values([\"R2\",\"RMSE\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nRegression Model Results:\")\n",
    "print(results_df)\n",
    "print(f\"\\nBest Model: {best['Name']} (R²={best['R2']:.3f})\")\n",
    "\n",
    "if model_errors:\n",
    "    print(f\"\\nModel Training Errors:\")\n",
    "    for model, error in model_errors.items():\n",
    "        print(f\"  • {model}: {error}\")\n",
    "    \n",
    "    # Provide recommendations based on common error patterns\n",
    "    error_messages = \" \".join(model_errors.values()).lower()\n",
    "    if \"memory\" in error_messages or \"memoryerror\" in error_messages:\n",
    "        print(\"\\nRecommendation: Consider reducing CHUNK_SIZE or MAX_MEMORY_GB in configuration\")\n",
    "    if \"singular matrix\" in error_messages or \"linearly dependent\" in error_messages:\n",
    "        print(\"Recommendation: Check for highly correlated features or constant columns\")\n",
    "    if \"sample\" in error_messages:\n",
    "        print(\"Recommendation: Increase training data or adjust train/test split ratio\")\n",
    "\n",
    "# --- Additional Models: Dryness Classifier & t+1 Forecaster ---\n",
    "\n",
    "# Dryness Classifier (using configurable threshold from config)\n",
    "try:\n",
    "    logger.info(f\"Training dryness classifier (threshold: {DRY_THRESHOLD}%)\")\n",
    "    \n",
    "    y_dry = (y < DRY_THRESHOLD).astype(int)\n",
    "    dry_ratio = y_dry.mean()\n",
    "    logger.info(f\"Dryness ratio: {dry_ratio:.1%} of samples below {DRY_THRESHOLD}%\")\n",
    "    \n",
    "    if dry_ratio == 0 or dry_ratio == 1:\n",
    "        logger.warning(f\"Extreme class imbalance: {dry_ratio:.1%} positive class\")\n",
    "\n",
    "    classifiers = {\n",
    "        \"RandomForest\": RandomForestClassifier(\n",
    "            n_estimators=CLF_N_ESTIMATORS, \n",
    "            max_depth=CLF_MAX_DEPTH, \n",
    "            random_state=RANDOM_STATE, \n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced',  # Handle class imbalance\n",
    "            oob_score=True\n",
    "        ),\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=10\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    clf_rows = []\n",
    "    best_clf = None\n",
    "    clf_errors = {}\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        try:\n",
    "            clf.fit(X.loc[train_idx], y_dry.loc[train_idx])\n",
    "            proba = clf.predict_proba(X.loc[test_idx])[:, 1]\n",
    "            pred = (proba >= 0.5).astype(int)\n",
    "            \n",
    "            acc = accuracy_score(y_dry.loc[test_idx], pred)\n",
    "            f1 = f1_score(y_dry.loc[test_idx], pred, zero_division=0)\n",
    "            \n",
    "            try:\n",
    "                auc = roc_auc_score(y_dry.loc[test_idx], proba)\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"AUC calculation failed for {name}: {e}\")\n",
    "                auc = float('nan')\n",
    "            \n",
    "            clf_rows.append({\"Model\": name, \"Accuracy\": acc, \"F1\": f1, \"AUC\": auc})\n",
    "            \n",
    "            # Track best classifier\n",
    "            score = auc if not np.isnan(auc) else f1\n",
    "            best_score = best_clf.get(\"AUC\" if not np.isnan(auc) else \"F1\", -1) if best_clf else -1\n",
    "            \n",
    "            if best_clf is None or score > best_score:\n",
    "                best_clf = {\"Name\": name, \"Model\": clf, \"Accuracy\": acc, \"F1\": f1, \"AUC\": auc}\n",
    "            \n",
    "            logger.info(f\"{name} Classifier: Acc={acc:.3f}, F1={f1:.3f}, AUC={auc:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to train {name} classifier: {e}\"\n",
    "            logger.error(error_msg)\n",
    "            clf_errors[name] = str(e)\n",
    "\n",
    "    if clf_rows:\n",
    "        clf_results_df = pd.DataFrame(clf_rows).sort_values([\"AUC\", \"F1\", \"Accuracy\"], \n",
    "                                                            ascending=[False, False, False]).reset_index(drop=True)\n",
    "        print(f\"\\nDryness Classifier Results (threshold: {DRY_THRESHOLD}%):\")\n",
    "        print(clf_results_df)\n",
    "        print(f\"Best Classifier: {best_clf['Name']}\")\n",
    "    else:\n",
    "        logger.error(\"All classifiers failed to train\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Dryness classification failed: {e}\")\n",
    "\n",
    "# t+1 Moisture Forecaster\n",
    "try:\n",
    "    logger.info(\"Training t+1 forecaster...\")\n",
    "    \n",
    "    df_base = features.copy()\n",
    "    site_cols = [c for c in df_base.columns if c.startswith(\"site_id_\")]\n",
    "    if site_cols:\n",
    "        df_base[\"site\"] = df_base[site_cols].idxmax(axis=1).str.replace(\"site_id_\", \"\", regex=False)\n",
    "    else:\n",
    "        df_base[\"site\"] = \"site\"\n",
    "\n",
    "    df_base[\"soil_moisture_pct\"] = raw.loc[df_base.index, \"soil_moisture_pct\"]\n",
    "\n",
    "    def shift_future_target(group):\n",
    "        group = group.sort_values(\"timestamp\").copy()\n",
    "        group[\"y_t1\"] = group[\"soil_moisture_pct\"].shift(-1)\n",
    "        return group\n",
    "\n",
    "    df_t1 = df_base.groupby(\"site\", group_keys=False).apply(shift_future_target)\n",
    "\n",
    "    X_t1 = df_t1[feat_cols].astype(float)\n",
    "    y_t1 = pd.to_numeric(df_t1[\"y_t1\"], errors=\"coerce\")\n",
    "    mask_t1 = X_t1.notna().all(axis=1) & y_t1.notna()\n",
    "    X_t1, y_t1 = X_t1[mask_t1], y_t1[mask_t1]\n",
    "\n",
    "    if len(X_t1) < 50:\n",
    "        raise ValueError(f\"Insufficient data for t+1 forecasting: {len(X_t1)} samples\")\n",
    "\n",
    "    # Use filtered data for t+1 split\n",
    "    features_t1_filtered = df_t1.loc[mask_t1]\n",
    "    train_idx_t1, test_idx_t1 = time_aware_split_indices(features_t1_filtered)\n",
    "    \n",
    "    # Filter indices to ensure they exist in X_t1\n",
    "    train_idx_t1 = [idx for idx in train_idx_t1 if idx in X_t1.index]\n",
    "    test_idx_t1 = [idx for idx in test_idx_t1 if idx in X_t1.index]\n",
    "\n",
    "    # Train t+1 forecaster\n",
    "    forecaster = GradientBoostingRegressor(\n",
    "        random_state=RANDOM_STATE,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10\n",
    "    )\n",
    "    \n",
    "    validate_model_inputs(X_t1.loc[train_idx_t1], y_t1.loc[train_idx_t1])\n",
    "    forecaster.fit(X_t1.loc[train_idx_t1], y_t1.loc[train_idx_t1])\n",
    "\n",
    "    pred_t1 = np.clip(forecaster.predict(X_t1.loc[test_idx_t1]), 0, 100)\n",
    "    \n",
    "    # Fix for older scikit-learn versions\n",
    "    mse_t1 = mean_squared_error(y_t1.loc[test_idx_t1], pred_t1)\n",
    "    rmse_t1 = np.sqrt(mse_t1)  # Calculate RMSE manually\n",
    "    mae_t1 = mean_absolute_error(y_t1.loc[test_idx_t1], pred_t1)\n",
    "    r2_t1 = r2_score(y_t1.loc[test_idx_t1], pred_t1)\n",
    "\n",
    "    logger.info(f\"t+1 Forecaster: RMSE={rmse_t1:.3f}, MAE={mae_t1:.3f}, R²={r2_t1:.3f}\")\n",
    "    print(f\"\\nt+1 Forecaster (Gradient Boosting) — RMSE: {rmse_t1:.3f}, MAE: {mae_t1:.3f}, R²: {r2_t1:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"t+1 forecasting failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model results as CSV\n",
    "results_df.to_csv(\"siams_model_results.csv\", index=False)\n",
    "print(\"Saved model results to: siams_model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8692bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predicted vs Actual (test set)\n",
    "plot_dir = Path(\"plots\")\n",
    "plot_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def plot_pred_actual(y_true, y_pred, title, fname):\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, s=8)\n",
    "    plt.xlabel(\"Actual Soil Moisture (%)\")\n",
    "    plt.ylabel(\"Predicted Soil Moisture (%)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_dir / fname, dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "models_dict = {\n",
    "    \"DecisionTree\": DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        n_estimators=RF_N_ESTIMATORS, max_depth=RF_MAX_DEPTH, min_samples_leaf=RF_MIN_SAMPLES, \n",
    "        random_state=RANDOM_STATE, n_jobs=-1\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=RANDOM_STATE),\n",
    "}\n",
    "if HAVE_XGB:\n",
    "    models_dict[\"XGBoost\"] = XGBRegressor(\n",
    "        n_estimators=XGB_N_ESTIMATORS, learning_rate=XGB_LEARNING_RATE, max_depth=XGB_MAX_DEPTH, \n",
    "        subsample=0.9, colsample_bytree=0.9, random_state=RANDOM_STATE, n_jobs=-1, \n",
    "        reg_lambda=1.0, tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "# Fit models on train and plot\n",
    "for name, mdl in models_dict.items():\n",
    "    mdl.fit(X.loc[train_idx], y.loc[train_idx])\n",
    "    y_pred = np.clip(mdl.predict(X.loc[test_idx]), 0, 100)\n",
    "    plot_pred_actual(y.loc[test_idx].values, y_pred, f\"{name}: Predicted vs Actual\", f\"pred_vs_actual_{name}.png\")\n",
    "\n",
    "print(\"Saved plots to:\", plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d03a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature Importance (tree-based models)\n",
    "def plot_importances(model, feature_names, title, fname):\n",
    "    if not hasattr(model, \"feature_importances_\"):\n",
    "        return\n",
    "    imp = model.feature_importances_\n",
    "    order = np.argsort(imp)[::-1][:20]\n",
    "    plt.figure(figsize=(8, max(4, len(order)*0.35)))\n",
    "    plt.barh(range(len(order)), imp[order])\n",
    "    plt.yticks(range(len(order)), [feature_names[i] for i in order])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_dir / fname, dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    plot_importances(model, feat_cols, f\"{name} — Feature Importance\", f\"feat_importance_{name}.png\")\n",
    "\n",
    "print(\"Saved feature importance plots to:\", plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b7718d",
   "metadata": {},
   "source": [
    "## 5. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36820bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best[\"Model\"].fit(X.loc[train_idx], y.loc[train_idx])\n",
    "\n",
    "# Save best regression model\n",
    "joblib.dump(best_model, \"model.joblib\")\n",
    "with open(\"expected_features.json\", \"w\") as f:\n",
    "    json.dump(feat_cols, f, indent=2)\n",
    "\n",
    "# Save dryness classifier if it exists\n",
    "if \"best_clf\" in globals() and best_clf and \"Model\" in best_clf:\n",
    "    best_clf_model = best_clf[\"Model\"].fit(X.loc[train_idx], (y < DRY_THRESHOLD).astype(int).loc[train_idx])\n",
    "    joblib.dump(best_clf_model, \"dryness_clf.joblib\")\n",
    "    with open(\"dryness_meta.json\", \"w\") as f:\n",
    "        json.dump({\"threshold\": float(DRY_THRESHOLD), \"features\": list(X.columns)}, f, indent=2)\n",
    "    print(\"Saved: dryness_clf.joblib, dryness_meta.json\")\n",
    "else:\n",
    "    print(\"Dryness classifier not available — skipped saving.\")\n",
    "\n",
    "# Save t+1 forecaster if it exists\n",
    "if \"forecaster\" in globals():\n",
    "    joblib.dump(forecaster, \"model_t1.joblib\")\n",
    "    with open(\"expected_features_t1.json\", \"w\") as f:\n",
    "        json.dump(list(X_t1.columns), f, indent=2)\n",
    "    print(\"Saved: model_t1.joblib, expected_features_t1.json\")\n",
    "else:\n",
    "    print(\"t+1 forecaster not available — skipped saving.\")\n",
    "\n",
    "# Save comprehensive context metadata for the Streamlit app's LLM integration\n",
    "context_meta = {\n",
    "    # Core system information\n",
    "    \"known_sites\": KNOWN_SITES,\n",
    "    \"dry_threshold_default\": float(DRY_THRESHOLD),\n",
    "    \n",
    "    # Feature information\n",
    "    \"feature_columns\": feat_cols,\n",
    "    \"health_metrics\": {\n",
    "        \"vpd_kpa\": \"Vapor Pressure Deficit (kPa) - plant water stress indicator\",\n",
    "        \"dew_point_c\": \"Dew point temperature (°C) - condensation risk\",\n",
    "        \"heat_flag\": f\"Heat stress flag (temperature ≥ {HEAT_STRESS_TEMP}°C)\",\n",
    "        \"frost_flag\": f\"Frost risk flag (temperature ≤ {FROST_RISK_TEMP}°C)\",\n",
    "        \"waterlog_flag\": f\"Waterlogging risk (soil moisture ≥ {WATERLOG_THRESHOLD}%)\",\n",
    "        \"dryspell_flag\": f\"Drought stress flag (soil moisture ≤ {DRYSPELL_THRESHOLD}%)\",\n",
    "        \"disease_risk\": \"Disease risk score (0-100) based on humidity, temperature, and VPD\",\n",
    "        \"sensor_issue_flag\": \"Sensor malfunction indicator (stuck readings detection)\"\n",
    "    },\n",
    "    \n",
    "    # Model performance information\n",
    "    \"model_info\": {\n",
    "        \"best_model\": best['Name'],\n",
    "        \"models_trained\": list(models.keys()) if 'models' in globals() else [],\n",
    "        \"training_samples\": len(train_idx),\n",
    "        \"test_samples\": len(test_idx),\n",
    "        \"best_r2\": float(best['R2']),\n",
    "        \"best_rmse\": float(best['RMSE']),\n",
    "        \"best_mae\": float(best['MAE']),\n",
    "        \"feature_count\": len(feat_cols),\n",
    "        \"health_feature_count\": len([c for c in HEALTH_COLS_TRAIN if c in feat_cols])\n",
    "    },\n",
    "    \n",
    "    # Dataset information\n",
    "    \"data_info\": {\n",
    "        \"total_records\": len(raw),\n",
    "        \"processed_records\": len(X),\n",
    "        \"data_quality\": f\"{len(X)/len(raw)*100:.1f}% usable after cleaning\",\n",
    "        \"date_range\": {\n",
    "            \"start\": str(raw[\"timestamp\"].min()),\n",
    "            \"end\": str(raw[\"timestamp\"].max())\n",
    "        },\n",
    "        \"sites_count\": len(raw[\"site_id\"].unique()) if \"site_id\" in raw.columns else 1,\n",
    "        \"sites_list\": sorted(raw[\"site_id\"].unique().tolist()) if \"site_id\" in raw.columns else [\"Unknown\"]\n",
    "    },\n",
    "    \n",
    "    # Configuration parameters used\n",
    "    \"config_params\": {\n",
    "        \"test_fraction\": TEST_FRACTION,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"lag_windows\": LAG_WINDOWS,\n",
    "        \"rolling_window\": ROLLING_WINDOW,\n",
    "        \"vpd_disease_threshold\": VPD_DISEASE_THRESHOLD,\n",
    "        \"heat_stress_temp\": HEAT_STRESS_TEMP,\n",
    "        \"frost_risk_temp\": FROST_RISK_TEMP,\n",
    "        \"waterlog_threshold\": WATERLOG_THRESHOLD,\n",
    "        \"dryspell_threshold\": DRYSPELL_THRESHOLD\n",
    "    },\n",
    "    \n",
    "    # Health metrics export info\n",
    "    \"health_export\": {\n",
    "        \"filename\": \"siams_health_metrics.csv\",\n",
    "        \"columns\": available_health_cols,\n",
    "        \"total_records\": len(health_export) if 'health_export' in globals() else 0\n",
    "    },\n",
    "    \n",
    "    # Additional models info\n",
    "    \"additional_models\": {\n",
    "        \"dryness_classifier\": {\n",
    "            \"available\": \"best_clf\" in globals() and best_clf is not None,\n",
    "            \"threshold\": float(DRY_THRESHOLD),\n",
    "            \"model_type\": best_clf['Name'] if 'best_clf' in globals() and best_clf else None\n",
    "        },\n",
    "        \"t1_forecaster\": {\n",
    "            \"available\": \"forecaster\" in globals(),\n",
    "            \"model_type\": \"GradientBoosting\" if \"forecaster\" in globals() else None\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Generation metadata\n",
    "    \"pipeline_info\": {\n",
    "        \"generated_at\": pd.Timestamp.now().isoformat(),\n",
    "        \"pipeline_version\": \"SIAMS_ML_v2.0_with_health_metrics\",\n",
    "        \"timezone\": \"Africa/Lagos\",\n",
    "        \"data_source\": MULTISHEET_XLSX or SHEET_CSV_URL or LOCAL_CSV or \"Unknown\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save context metadata\n",
    "with open(\"context_meta.json\", \"w\") as f:\n",
    "    json.dump(context_meta, f, indent=2)\n",
    "\n",
    "print(\"Saved: model.joblib, expected_features.json\")\n",
    "print(\"Saved: context_meta.json (comprehensive system metadata)\")\n",
    "print(f\"Context metadata includes {len(context_meta)} main categories:\")\n",
    "for category in context_meta.keys():\n",
    "    print(f\"  • {category}\")\n",
    "\n",
    "logger.info(\"All artifacts saved successfully with enhanced metadata support\")\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
